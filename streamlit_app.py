# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YU4Q81VWpTjfOrEW0JWUK5087iAjd3Vs
"""

# financial_qa_app.py

import pandas as pd
import time
import os
from pathlib import Path
!pip install streamlit
import streamlit as st
!pip install --upgrade pip




# NLP libraries
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Generative model placeholder
from transformers import AutoModelForCausalLM, AutoTokenizer

# -----------------------------
# ---------- CONFIG ------------
# -----------------------------
EMBEDDING_MODEL = "all-MiniLM-L6-v2"   # for RAG embeddings
GEN_MODEL_NAME = "distilgpt2"          # small generative model
CHUNK_SIZE = 200                        # tokens/chars
TOP_N = 3                               # top chunks to retrieve

# -----------------------------
# ---------- DATA PREP ----------
# -----------------------------
@st.cache_data
def load_financial_data(file_path: str):
    """
    Load PDF, Excel or CSV, convert to text
    For simplicity, we assume CSV with 'section' and 'text'
    """
    df = pd.read_csv(file_path)
    return df

@st.cache_data
def chunk_text(df, chunk_size=CHUNK_SIZE):
    """
    Split each section into chunks
    """
    chunks = []
    for idx, row in df.iterrows():
        text = row['text']
        section = row['section']
        for i in range(0, len(text), chunk_size):
            chunk_text = text[i:i+chunk_size]
            chunks.append({
                "id": f"{idx}_{i}",
                "section": section,
                "text": chunk_text
            })
    return chunks

# -----------------------------
# ---------- RAG SYSTEM --------
# -----------------------------
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(EMBEDDING_MODEL)

def embed_chunks(chunks, model):
    texts = [c['text'] for c in chunks]
    embeddings = model.encode(texts, convert_to_numpy=True)
    return embeddings

def rag_retrieve(query, chunks, embeddings, tfidf_vectorizer=None, tfidf_matrix=None, top_n=TOP_N):
    """
    Hybrid retrieval: Dense (embedding) + Sparse (TF-IDF) combination
    """
    query_emb = embedding_model.encode([query], convert_to_numpy=True)
    # Dense retrieval
    dense_scores = cosine_similarity(query_emb, embeddings)[0]
    dense_top_idx = dense_scores.argsort()[-top_n:][::-1]

    # Sparse retrieval
    sparse_top_idx = []
    if tfidf_vectorizer and tfidf_matrix is not None:
        query_vec = tfidf_vectorizer.transform([query])
        sparse_scores = cosine_similarity(query_vec, tfidf_matrix)[0]
        sparse_top_idx = sparse_scores.argsort()[-top_n:][::-1]

    # Combine indices
    combined_idx = list(set(dense_top_idx.tolist() + sparse_top_idx.tolist()))
    retrieved_chunks = [chunks[i]['text'] for i in combined_idx]

    return retrieved_chunks

def rag_generate_answer(query, retrieved_chunks, gen_model, gen_tokenizer, max_input_tokens=512):
    """
    Concatenate retrieved chunks and query, then generate answer
    """
    context = " ".join(retrieved_chunks)[:max_input_tokens]
    input_text = f"Context: {context}\nQuestion: {query}\nAnswer:"

    inputs = gen_tokenizer(input_text, return_tensors="pt")
    outputs = gen_model.generate(**inputs, max_length=150)
    answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer

# -----------------------------
# ---------- FINE-TUNED --------
# -----------------------------
# Placeholder: you can replace with actual fine-tuned model loading & inference
def ft_answer(query):
    """
    Dummy fine-tuned answer
    """
    answers = {
        "revenue 2023": "The revenue in 2023 was $4.13B.",
        "assets 2022": "Total assets for 2022: $2.8B"
    }
    # simple keyword matching
    query_lower = query.lower()
    for key in answers:
        if key in query_lower:
            return answers[key], 0.93, "Fine-Tuned GPT-2 Small"
    return "Answer not in dataset", 0.5, "Fine-Tuned GPT-2 Small"

# -----------------------------
# ---------- STREAMLIT UI ------
# -----------------------------
st.set_page_config(page_title="Financial Q&A Chatbot", layout="wide")
st.title("ðŸ’° Financial Q&A Chatbot: RAG vs Fine-Tuned")

# Sidebar: Model selection
model_choice = st.sidebar.radio("Select Model:", ("RAG", "Fine-Tuned"))

# Sidebar: Upload data
uploaded_file = st.sidebar.file_uploader("Upload financial CSV (columns: section,text)", type=["csv"])
chunks = []
embeddings = None
tfidf_vectorizer = None
tfidf_matrix = None
embedding_model = None

if uploaded_file:
    st.sidebar.success("File uploaded!")
    df = load_financial_data(uploaded_file)
    chunks = chunk_text(df)
    st.sidebar.write(f"Total chunks created: {len(chunks)}")

    if model_choice == "RAG":
        st.sidebar.info("Loading embedding & generative models...")
        embedding_model = load_embedding_model()
        embeddings = embed_chunks(chunks, embedding_model)

        # Sparse TF-IDF
        tfidf_vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf_vectorizer.fit_transform([c['text'] for c in chunks])

        gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)
        gen_model = AutoModelForCausalLM.from_pretrained(GEN_MODEL_NAME)

# User query input
query = st.text_input("Enter your financial question:")

# Button to get answer
if st.button("Get Answer") and query:
    start_time = time.time()

    if model_choice == "RAG":
        if not chunks:
            st.error("Please upload financial data CSV first!")
        else:
            retrieved_chunks = rag_retrieve(query, chunks, embeddings, tfidf_vectorizer, tfidf_matrix)
            answer = rag_generate_answer(query, retrieved_chunks, gen_model, gen_tokenizer)
            confidence = 0.9  # dummy confidence
            method_used = "Hybrid Retrieval + DistilGPT2"
    else:
        answer, confidence, method_used = ft_answer(query)

    end_time = time.time()
    response_time = round(end_time - start_time, 2)

    # Display results
    st.markdown("### âœ… Answer")
    st.write(answer)

    st.markdown("### ðŸ“Š Details")
    st.write(f"**Confidence Score:** {confidence}")
    st.write(f"**Method Used:** {method_used}")
    st.write(f"**Inference Time (s):** {response_time}")

# Example questions
st.markdown("### Example Questions")
st.write("- What was the companyâ€™s revenue in 2023?")
st.write("- How many unique products were sold in 2023?")
st.write("- What is the total assets value for 2022?")
st.write("- Capital of France? (irrelevant question for robustness)")
